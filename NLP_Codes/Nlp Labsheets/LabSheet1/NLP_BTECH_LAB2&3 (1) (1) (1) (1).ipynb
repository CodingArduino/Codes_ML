{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bee5e0cf",
      "metadata": {
        "id": "bee5e0cf"
      },
      "source": [
        "# Various Steps in NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4ec4ac2",
      "metadata": {
        "id": "c4ec4ac2"
      },
      "source": [
        "We will be using a Python library called NLTK (Natural Language Toolkit).\n",
        "\n",
        "NLTK is a powerful open source tool that provides a set of methods and algorithms to perform a wide range of NLP tasks, including tokenizing, parts-of-speech tagging, stemming, lemmatization, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15216e47",
      "metadata": {
        "id": "15216e47"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "015d7d6e",
      "metadata": {
        "id": "015d7d6e"
      },
      "source": [
        "Tokenization refers to the procedure of splitting a sentence into its constituent parts—the words and punctuation that it is made up of.\n",
        "\n",
        "NLTK provides a method called word_tokenize(), which tokenizes given text into words.\n",
        "\n",
        "It actually separates the text into different words based on punctuation and spaces between words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e3b374b",
      "metadata": {
        "id": "4e3b374b"
      },
      "source": [
        "Import the necessary libraries and download the different types of NLTK data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae551ca2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae551ca2",
        "outputId": "38435420-4cf5-4868-ab5d-413468929cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from nltk import word_tokenize, download\n",
        "download(['punkt','averaged_perceptron_tagger','stopwords'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83e696bc",
      "metadata": {
        "id": "83e696bc"
      },
      "source": [
        "We need to add a sentence as input to the word_tokenize() method so that it performs its job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f7e370",
      "metadata": {
        "id": "d8f7e370"
      },
      "outputs": [],
      "source": [
        "def get_tokens(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e96d4755",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e96d4755",
        "outputId": "28494e8f-3d2e-4dca-c995-1f769e3b401b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'reading', 'NLP', 'Fundamentals', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "print(get_tokens(\"I am reading NLP Fundamentals.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e901e784",
      "metadata": {
        "id": "e901e784"
      },
      "source": [
        "## PoS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c679ff1",
      "metadata": {
        "id": "7c679ff1"
      },
      "source": [
        "PoS refers to parts of speech.\n",
        "\n",
        "PoS tagging refers to the process of tagging words within sentences with their respective PoS."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ee139f",
      "metadata": {
        "id": "61ee139f"
      },
      "source": [
        "import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb5abdff",
      "metadata": {
        "id": "cb5abdff"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize, pos_tag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b5ff9f3",
      "metadata": {
        "id": "2b5ff9f3"
      },
      "source": [
        "Using word_tokenize() method, find the tokens in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0929114",
      "metadata": {
        "id": "b0929114"
      },
      "outputs": [],
      "source": [
        "def get_tokens(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "868e5a70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "868e5a70",
        "outputId": "2530474c-10c4-4295-d395-ecde12c5ce55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'reading', 'NLP', 'Fundamentals']\n"
          ]
        }
      ],
      "source": [
        "words  = get_tokens(\"I am reading NLP Fundamentals\")\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba21f789",
      "metadata": {
        "id": "ba21f789"
      },
      "source": [
        "Use the pos_tag() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b21447db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b21447db",
        "outputId": "2e4c53ca-56bd-45ac-abd1-738c3d2a854b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('reading', 'VBG'),\n",
              " ('NLP', 'NNP'),\n",
              " ('Fundamentals', 'NNS')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "def get_pos(words):\n",
        "    return pos_tag(words)\n",
        "get_pos(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b9f97d",
      "metadata": {
        "id": "25b9f97d"
      },
      "source": [
        "PRP stands for personal pronoun.\n",
        "\n",
        "VBP stands for verb present.\n",
        "\n",
        "VGB stands for verb gerund.\n",
        "\n",
        "NNP stands for proper noun singular\n",
        "\n",
        "NNS stands for noun plural."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7966525",
      "metadata": {
        "id": "d7966525"
      },
      "source": [
        "## Stop Word Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b7f564e",
      "metadata": {
        "id": "6b7f564e"
      },
      "source": [
        "Stop words are the most frequently occurring words in any language.\n",
        "\n",
        "They are just used to support the construction of sentences and do not contribute anything to the semantics of a sentence.\n",
        "\n",
        "Removing them will help us clean our data, making its analysis much more efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f3eedc8",
      "metadata": {
        "id": "8f3eedc8"
      },
      "source": [
        "Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea4d96f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea4d96f2",
        "outputId": "7620221b-f9c3-41ac-a01f-c7f4ef34d918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import download\n",
        "download('stopwords')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fcf1b75",
      "metadata": {
        "id": "0fcf1b75"
      },
      "source": [
        "In order to check the list of stop words provided for English, we pass it as a parameter to the words() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "048baf98",
      "metadata": {
        "id": "048baf98"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd9e7406",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd9e7406",
        "outputId": "be9ede9b-bebd-4062-a1a5-c7ed9ce4929d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1538196c",
      "metadata": {
        "id": "1538196c"
      },
      "source": [
        "To remove the stop words from a sentence-\n",
        "\n",
        "Assign a string to the sentence variable.\n",
        "\n",
        "Tokenize it into words using the word_tokenize() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c4e808",
      "metadata": {
        "id": "32c4e808"
      },
      "outputs": [],
      "source": [
        "sentence = \"I am learning Python. It is one of the \"\\\n",
        "\"most popular programming languages\"\n",
        "sentence_words = word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b032a00c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b032a00c",
        "outputId": "7dd58f5e-70c8-445b-f489-17bc70ba9b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'learning', 'Python', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'programming', 'languages']\n"
          ]
        }
      ],
      "source": [
        "print(sentence_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521196af",
      "metadata": {
        "id": "521196af"
      },
      "source": [
        "To remove the stop words, we need to loop through each word in the sentence, check whether there are any stop words, and then finally combine them to form a complete sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcf8f8b7",
      "metadata": {
        "id": "fcf8f8b7"
      },
      "outputs": [],
      "source": [
        "def remove_stop_words(sentence_words, stop_words):\n",
        "    return ' '.join([word for word in sentence_words if \\\n",
        "                     word not in stop_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85e1dcee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85e1dcee",
        "outputId": "5a7ac4d2-a994-4760-d8e6-4517f31246df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I learning Python . It one popular programming languages\n"
          ]
        }
      ],
      "source": [
        "print(remove_stop_words(sentence_words,stop_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df89269",
      "metadata": {
        "id": "6df89269"
      },
      "source": [
        "Add your own stop words to the stop word list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2930147d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2930147d",
        "outputId": "f019f2be-6e1a-475b-8980-12a0bb2264cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning Python . popular programming languages\n"
          ]
        }
      ],
      "source": [
        "stop_words.extend(['I','It', 'one'])\n",
        "print(remove_stop_words(sentence_words,stop_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e82075",
      "metadata": {
        "id": "c0e82075"
      },
      "source": [
        "## Text Normalization"
      ]
    },
    {
      "cell_type": "raw",
      "id": "60dec520",
      "metadata": {
        "id": "60dec520"
      },
      "source": [
        "Text normalization is a process wherein different variations of text get converted into a standard form.\n",
        "For example, words such as \"does\" and \"doing,\" when converted to their base form, become \"do.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f56963b",
      "metadata": {
        "id": "1f56963b"
      },
      "source": [
        "There are various ways of normalizing text-\n",
        "\n",
        "1. spelling correction\n",
        "2. stemming\n",
        "3. lemmatization"
      ]
    },
    {
      "cell_type": "raw",
      "id": "c9169a29",
      "metadata": {
        "id": "c9169a29"
      },
      "source": [
        "We will be trying to replace select words with new words, using the replace() function, and finally produce the normalized text.\n",
        "replace() is a built-in Python function that works on strings and takes two arguments.\n",
        "It will return a copy of a string in which the occurrence of the first argument will be replaced by the second argument."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abd886ce",
      "metadata": {
        "id": "abd886ce"
      },
      "source": [
        "Assign a string to the sentence variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3778e61",
      "metadata": {
        "id": "a3778e61"
      },
      "outputs": [],
      "source": [
        "sentence = \"I visited the US from the UK on 22-10-18\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "724dba84",
      "metadata": {
        "id": "724dba84"
      },
      "source": [
        "Replace-\n",
        "1. \"US\" with \"United States\"\n",
        "2. \"UK\" with \"United Kingdom\"\n",
        "3. \"18\" with \"2018\"\n",
        "\n",
        "To do so, use the replace() function and store the updated output in the \"normalized_sentence\" variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c538e232",
      "metadata": {
        "id": "c538e232"
      },
      "outputs": [],
      "source": [
        "def normalize(text):\n",
        "    return text.replace(\"US\", \"United States\")\\\n",
        ".replace(\"UK\", \"United Kingdom\")\\\n",
        ".replace(\"-18\", \"-2018\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6476efe",
      "metadata": {
        "id": "d6476efe"
      },
      "source": [
        "Check whether the text has been normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71fa1938",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71fa1938",
        "outputId": "df13c783-7232-49df-8bf8-4cec95fd87cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I visited the United States from the United Kingdom on 22-10-2018\n"
          ]
        }
      ],
      "source": [
        "normalized_sentence = normalize(sentence)\n",
        "print(normalized_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1204a27a",
      "metadata": {
        "id": "1204a27a"
      },
      "source": [
        "## Spelling Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9bf510a",
      "metadata": {
        "id": "e9bf510a"
      },
      "source": [
        "Spelling correction is one of the most important tasks in any NLP project."
      ]
    },
    {
      "cell_type": "raw",
      "id": "1a01a831",
      "metadata": {
        "id": "1a01a831"
      },
      "source": [
        "Spelling correction is executed in two steps:\n",
        "\n",
        "1. Identify the misspelled word, which can be done by a simple dictionary lookup.\n",
        "   If there is no match found in the language dictionary, it is considered to be misspelled.\n",
        "\n",
        "2. Replace it or suggest the correctly spelled word.\n",
        "   There are a lot of algorithms for this task.\n",
        "   One of them is the minimum edit distance algorithm, which chooses the nearest correctly spelled word for a misspelled word.    The nearness is defined by the number of edits that need to be made to the misspelled word to reach the correctly spelled word.\n",
        "   For example, let's say there is a misspelled word, \"autocorect.\" Now, to make it \"autocorrect,\" we need to add one \"r,\" and to make it \"auto,\" we need to delete 6 characters, which means that \"autocorrect\" is the correct spelling because it requires the fewest edits."
      ]
    },
    {
      "cell_type": "raw",
      "id": "2bed6570",
      "metadata": {
        "id": "2bed6570"
      },
      "source": [
        "autocorrect is a Python library used to correct the spelling of misspelled words for different languages.\n",
        "It provides a method called spell(), which takes a word as input and returns the correct spelling of the word."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ed23e5d",
      "metadata": {
        "id": "1ed23e5d"
      },
      "source": [
        "Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37ca94f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ca94f2",
        "outputId": "e0ae659d-c577-45e4-b2ce-36a81eba10b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/622.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622364 sha256=970a0a8cca10e12496d78a980150209f91579c6a7f41c8e22defc5010fd16073\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/90/99/807a5ad861ce5d22c3c299a11df8cba9f31524f23ae6e645cb\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install autocorrect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d106d84f",
      "metadata": {
        "id": "d106d84f"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize\n",
        "from autocorrect import Speller"
      ]
    },
    {
      "cell_type": "raw",
      "id": "aff12e42",
      "metadata": {
        "id": "aff12e42"
      },
      "source": [
        "In order to correct the spelling of a word, pass a wrongly spelled word as a parameter to the spell() function.\n",
        "Before that, you have to create a spell object of the Speller class using lang='en' to signify the English language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba46751",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fba46751",
        "outputId": "ad740317-2e19-4c83-cd95-04d477d9c0df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Natural'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "spell = Speller(lang='en')\n",
        "spell('Natureal')"
      ]
    },
    {
      "cell_type": "raw",
      "id": "23c824b4",
      "metadata": {
        "id": "23c824b4"
      },
      "source": [
        "To correct the spelling of a sentence, first tokenize it into tokens.\n",
        "After that, loop through each token in sentence, autocorrect the words, and finally combine the words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c37fd218",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c37fd218",
        "outputId": "cd300eda-933a-4b3d-e8ca-24ff5eac181d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ntural', 'Luanguage', 'Processin', 'deals', 'with', 'the', 'art', 'of', 'extracting', 'insightes', 'from', 'Natural', 'Languaes']\n"
          ]
        }
      ],
      "source": [
        "sentence = word_tokenize(\"Ntural Luanguage Processin deals with \"\\\n",
        "\"the art of extracting insightes from \"\\\n",
        "\"Natural Languaes\")\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "30bcd241",
      "metadata": {
        "id": "30bcd241"
      },
      "source": [
        "Loop through each token in sentence, correct the tokens, and assign them to a new variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb215c48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb215c48",
        "outputId": "f40981d9-1f5c-4aff-d566-4d461ecc65db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing deals with the art of extracting insights from Natural Languages\n"
          ]
        }
      ],
      "source": [
        "def correct_spelling(tokens):\n",
        "    sentence_corrected = ' '.join([spell(word) \\\n",
        "for word in tokens])\n",
        "    return sentence_corrected\n",
        "print(correct_spelling(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1bbc57f",
      "metadata": {
        "id": "a1bbc57f"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "raw",
      "id": "3e3d4383",
      "metadata": {
        "id": "3e3d4383"
      },
      "source": [
        "In most languages, words get transformed into various forms when being used in a sentence.\n",
        "For example, the word \"product\" might get transformed into \"production\" when referring to the process of making something or transformed into \"products\" in plural form.\n",
        "It is necessary to convert these words into their base forms, as they carry the same meaning in any case."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a33f88",
      "metadata": {
        "id": "21a33f88"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "a918e39e",
      "metadata": {
        "id": "a918e39e"
      },
      "source": [
        "We will be using two algorithms which are provided by the NLTK library-\n",
        "1. porter stemmer\n",
        "2. snowball stemmer\n",
        "The porter stemmer is a rule-based algorithm that transforms words to their base form by removing suffixes from words.\n",
        "The snowball stemmer is an improvement over the porter stemmer and is a little bit faster and uses less memory.\n",
        "In NLTK, this is done by the stem() method provided by the PorterStemmer class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ca35dae",
      "metadata": {
        "id": "7ca35dae"
      },
      "source": [
        "Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab9cab8",
      "metadata": {
        "id": "8ab9cab8"
      },
      "outputs": [],
      "source": [
        "from nltk import stem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c0315d9",
      "metadata": {
        "id": "9c0315d9"
      },
      "source": [
        " Pass the words as parameters to the stem() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a589711",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7a589711",
        "outputId": "e57d022a-de0d-4828-d863-bcad63637b45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'product'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "def get_stems(word,stemmer):\n",
        "    return stemmer.stem(word)\n",
        "porterStem = stem.PorterStemmer()\n",
        "get_stems(\"production\",porterStem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a637d33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1a637d33",
        "outputId": "0369f431-479f-4d75-ecf6-e1c3a436ced8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'come'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "get_stems(\"coming\",porterStem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa60f0e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fa60f0e0",
        "outputId": "763708ca-95dd-41ca-e9ce-8b44bc658bc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fire'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "get_stems(\"firing\",porterStem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70d6f642",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "70d6f642",
        "outputId": "9c207535-dd20-43d8-ef56-260709bc8474"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'battl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "get_stems(\"battling\",porterStem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3238681e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3238681e",
        "outputId": "ad2040d2-ea33-470d-ed92-f6677a521e33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'battl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "stemmer = stem.SnowballStemmer(\"english\")\n",
        "get_stems(\"battling\",stemmer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6643e5b0",
      "metadata": {
        "id": "6643e5b0"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "raw",
      "id": "b40d626d",
      "metadata": {
        "id": "b40d626d"
      },
      "source": [
        "The stemming processleads to incorrect results.\n",
        "For example, in the last exercise, the word battling was transformed to \"battl\", which is not a word."
      ]
    },
    {
      "cell_type": "raw",
      "id": "e3dcd4e4",
      "metadata": {
        "id": "e3dcd4e4"
      },
      "source": [
        "Lemmatization is the process of converting words to their base grammatical form, as in \"battling\" to \"battle,\" rather than just randomly axing words.\n",
        "In this process, an additional check is made by looking through a dictionary to extract the base form of a word.\n",
        "Getting more accurate results requires some additional information.\n",
        "For example, PoS tags along with words will help in getting better results."
      ]
    },
    {
      "cell_type": "raw",
      "id": "5c21ce5c",
      "metadata": {
        "id": "5c21ce5c"
      },
      "source": [
        "We will use WordNetLemmatizer, which is an NLTK interface of WordNet to do the Lemmatization.\n",
        "WordNet is a freely available lexical English database that can be used to generate semantic relationships between words. NLTK's WordNetLemmatizer provides a method called lemmatize(), which returns the lemma (grammatical base form) of a given word using WordNet."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b77ceccb",
      "metadata": {
        "id": "b77ceccb"
      },
      "source": [
        "Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa18f88c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa18f88c",
        "outputId": "fd7beda8-ba4d-4e65-fc42-8f714b8dc11e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from nltk import download\n",
        "download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdd099f9",
      "metadata": {
        "id": "fdd099f9"
      },
      "source": [
        "Create an object of the WordNetLemmatizer class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d1148a1",
      "metadata": {
        "id": "3d1148a1"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f16c8c5",
      "metadata": {
        "id": "1f16c8c5"
      },
      "source": [
        "Bring the word to its proper form by using the lemmatize() method of the WordNetLemmatizer class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df33471",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7df33471",
        "outputId": "d4f702e8-c36c-4ca1-c793-b4c2af813180"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'product'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "def get_lemma(word):\n",
        "    return lemmatizer.lemmatize(word)\n",
        "get_lemma('products')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "525261c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "525261c0",
        "outputId": "b7e6fc01-d472-46db-f235-17b035368032"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'production'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "get_lemma('production')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aea445a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1aea445a",
        "outputId": "d5fc7887-168c-4672-ab4e-8a3ec5b13776"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'coming'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "get_lemma('coming')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8702975f",
      "metadata": {
        "id": "8702975f"
      },
      "source": [
        "## Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "cc11b671",
      "metadata": {
        "id": "cc11b671"
      },
      "source": [
        "NER is the process of extracting important entities, such as person names, place names, and organization names, from some given text.\n",
        "These are usually not present in dictionaries.\n",
        "\n",
        "The main objective of this process is to identify the named entities (such as proper nouns) and map them to categories, which are already defined.\n",
        "For example, categories might include names of people, places, and so on.\n",
        "\n",
        "NER has found use in many NLP tasks, including assigning tags to news articles, search algorithms, and more.\n",
        "NER can analyze a news article and extract the major people, organizations, and places discussed in it and assign them as tags for new articles.\n",
        "\n",
        "In the case of search algorithms, let's suppose we have to create a search engine, meant specifically for books.\n",
        "If we were to submit a given query for all the words, the search would take a lot of time.\n",
        "Instead, if we extract the top entities from all the books using NER and run a search query on the entities rather than all the content, the speed of the system would increase dramatically.\n",
        "\n",
        "To get a better understanding of this process, we'll perform an exercise. Before moving on to the exercise, let me introduce you to chunking, which we are going to use in the following exercise. Chunking is the process of grouping words together into chunks, which can be further used to find noun groups and verb groups, or can also be used for sentence partitioning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aec32468",
      "metadata": {
        "id": "aec32468"
      },
      "source": [
        "Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47e5e63a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47e5e63a",
        "outputId": "fe3e1929-38be-480c-9eee-361c70683b9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "from nltk import download\n",
        "from nltk import pos_tag\n",
        "from nltk import ne_chunk\n",
        "from nltk import word_tokenize\n",
        "download('maxent_ne_chunker')\n",
        "download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a38e63e6",
      "metadata": {
        "id": "a38e63e6"
      },
      "source": [
        "Declare the <b>sentence</b>  variable and assign it a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "894cfc81",
      "metadata": {
        "id": "894cfc81"
      },
      "outputs": [],
      "source": [
        "sentence = \"We are reading a book published by Packt \"\\\n",
        "\"which is based out of Birmingham.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec1c3b39",
      "metadata": {
        "id": "ec1c3b39"
      },
      "source": [
        "Find the named entities from the preceding text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3306222c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3306222c",
        "outputId": "8ebf9733-02d8-49ec-b20d-65cd083d8944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tree('NE', [('Packt', 'NNP')]), Tree('NE', [('Birmingham', 'NNP')])]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "def get_ner(text):\n",
        "    i = ne_chunk(pos_tag(word_tokenize(text)), binary=True)\n",
        "    return [a for a in i if len(a)==1]\n",
        "get_ner(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e40d9140",
      "metadata": {
        "id": "e40d9140"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}